{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020.03.28 (Sat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model: spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded model 'models/en_core_web_lg_no_ner'\u001b[0m\n",
      "Created and merged data for 86 total examples\n",
      "Using 43 train / 43 eval (split 50%)\n",
      "Component: ner | Batch size: compounding | Dropout: 0.2 | Iterations: 10\n",
      "\u001b[38;5;4mℹ Baseline accuracy: 0.000\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== ✨  Training the model ===========================\u001b[0m\n",
      "\n",
      "#    Loss       Precision   Recall     F-Score \n",
      "--   --------   ---------   --------   --------\n",
      " 1    2260.23       3.822      5.714      4.580                                 \n",
      " 2     304.20      96.203     72.381     82.609                                 \n",
      " 3      35.94      93.069     89.524     91.262                                 \n",
      " 4       7.71      91.262     89.524     90.385                                 \n",
      " 5       6.89      94.000     89.524     91.707                                 \n",
      " 6       6.26      94.949     89.524     92.157                                 \n",
      " 7       4.15      94.000     89.524     91.707                                 \n",
      " 8      10.58      92.157     89.524     90.821                                 \n",
      " 9       1.78      92.157     89.524     90.821                                 \n",
      "10       0.00      93.069     89.524     91.262                                 \n",
      "\u001b[1m\n",
      "============================= ✨  Results summary =============================\u001b[0m\n",
      "\n",
      "Label         Precision   Recall   F-Score\n",
      "-----------   ---------   ------   -------\n",
      "RISK_FACTOR      94.949   89.524    92.157\n",
      "\n",
      "\n",
      "Best F-Score   \u001b[38;5;2m92.157\u001b[0m\n",
      "Baseline       0.000              \n",
      "\n",
      "\u001b[38;5;2m✔ Saved model:\n",
      "/home/users/grzenkom/git/GitHub-chopeen/CORD-19/models/2020_03_28_match/en_rf_web_lg\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!prodigy train ner cord_19_abstracts_match models/en_core_web_lg_no_ner --output models/2020_03_28_match/en_rf_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model: scispaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded model 'models/en_core_sci_lg_no_ner'\u001b[0m\n",
      "Created and merged data for 86 total examples\n",
      "Using 43 train / 43 eval (split 50%)\n",
      "Component: ner | Batch size: compounding | Dropout: 0.2 | Iterations: 10\n",
      "\u001b[38;5;4mℹ Baseline accuracy: 0.000\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== ✨  Training the model ===========================\u001b[0m\n",
      "\n",
      "#    Loss       Precision   Recall     F-Score \n",
      "--   --------   ---------   --------   --------\n",
      " 1    1864.33       3.333      2.941      3.125                                 \n",
      " 2     303.79      93.902     75.490     83.696                                 \n",
      " 3      22.39      92.784     88.235     90.452                                 \n",
      " 4      17.47      94.792     89.216     91.919                                 \n",
      " 5       5.36      93.814     89.216     91.457                                 \n",
      " 6       6.78      92.857     89.216     91.000                                 \n",
      " 7       6.09      93.814     89.216     91.457                                 \n",
      " 8       4.87      94.792     89.216     91.919                                 \n",
      " 9       9.64      93.814     89.216     91.457                                 \n",
      "10       4.02      94.792     89.216     91.919                                 \n",
      "\u001b[1m\n",
      "============================= ✨  Results summary =============================\u001b[0m\n",
      "\n",
      "Label         Precision   Recall   F-Score\n",
      "-----------   ---------   ------   -------\n",
      "RISK_FACTOR      94.792   89.216    91.919\n",
      "\n",
      "\n",
      "Best F-Score   \u001b[38;5;2m91.919\u001b[0m\n",
      "Baseline       0.000              \n",
      "\n",
      "\u001b[38;5;2m✔ Saved model:\n",
      "/home/users/grzenkom/git/GitHub-chopeen/CORD-19/models/2020_03_28_match/en_rf_sci_lg\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!prodigy train ner cord_19_abstracts_match models/en_core_sci_lg_no_ner --output models/2020_03_28_match/en_rf_sci_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020.03.29 (Sun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model trained on Sat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "def test_model(model_path):\n",
    "    warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "    \n",
    "    nlp = spacy.load(model_path)\n",
    "    texts = [\n",
    "    \"Known risk factors for the disease are: older age, male gender, diabetes and leukemia. Female patiens and children are less susceptible.\",\n",
    "    \"Diabetes is a known risk factor.\",\n",
    "    \"Leukemia is a risk factor, too.\"\n",
    "    ]\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "\n",
    "        spacy.displacy.render(doc, style='ent', jupyter=True)\n",
    "        pprint([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Known risk factors for the disease are: older \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    age\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">RISK_FACTOR</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    male\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">RISK_FACTOR</span>\n",
       "</mark>\n",
       " gender, \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    diabetes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">RISK_FACTOR</span>\n",
       "</mark>\n",
       " and leukemia. Female patiens and children are less susceptible.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('age', 'RISK_FACTOR'), ('male', 'RISK_FACTOR'), ('diabetes', 'RISK_FACTOR')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Diabetes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">RISK_FACTOR</span>\n",
       "</mark>\n",
       " is a known risk factor.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Diabetes', 'RISK_FACTOR')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Leukemia is a risk factor, too.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "test_model(\"models/2020_03_28_match/en_rf_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the factors that appeared during the training are detected. _Leukemia_ is **not highlighed** and that means that the NER model did not learn to recognize risk factors based on the sentence structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train new models after the `teach` session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded model 'models/en_core_web_lg_no_ner'\u001b[0m\n",
      "Created and merged data for 393 total examples\n",
      "Using 197 train / 196 eval (split 50%)\n",
      "Component: ner | Batch size: compounding | Dropout: 0.2 | Iterations: 10\n",
      "\u001b[38;5;4mℹ Baseline accuracy: 0.000\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== ✨  Training the model ===========================\u001b[0m\n",
      "\n",
      "#    Loss       Precision   Recall     F-Score \n",
      "--   --------   ---------   --------   --------\n",
      " 1     333.36      89.669     92.735     91.176                                 \n",
      " 2      64.66      92.982     90.598     91.775                                 \n",
      " 3      53.47      94.492     95.299     94.894                                 \n",
      " 4      45.26      94.492     95.299     94.894                                 \n",
      " 5      29.42      94.958     96.581     95.763                                 \n",
      " 6      30.44      96.170     96.581     96.375                                 \n",
      " 7      23.82      95.745     96.154     95.949                                 \n",
      " 8       5.78      95.319     95.726     95.522                                 \n",
      " 9       4.12      95.299     95.299     95.299                                 \n",
      "10       5.46      96.137     95.726     95.931                                 \n",
      "\u001b[1m\n",
      "============================= ✨  Results summary =============================\u001b[0m\n",
      "\n",
      "Label         Precision   Recall   F-Score\n",
      "-----------   ---------   ------   -------\n",
      "RISK_FACTOR      96.170   96.581    96.375\n",
      "\n",
      "\n",
      "Best F-Score   \u001b[38;5;2m96.375\u001b[0m\n",
      "Baseline       0.000              \n",
      "\n",
      "\u001b[38;5;2m✔ Saved model:\n",
      "/home/users/grzenkom/git/GitHub-chopeen/CORD-19/models/2020_03_29_teach/en_rf_web_lg\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!prodigy train ner cord_19_abstracts_teach models/en_core_web_lg_no_ner --output models/2020_03_29_teach/en_rf_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded model 'models/en_core_sci_lg_no_ner'\u001b[0m\n",
      "Created and merged data for 393 total examples\n",
      "Using 197 train / 196 eval (split 50%)\n",
      "Component: ner | Batch size: compounding | Dropout: 0.2 | Iterations: 10\n",
      "\u001b[38;5;4mℹ Baseline accuracy: 0.000\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== ✨  Training the model ===========================\u001b[0m\n",
      "\n",
      "#    Loss       Precision   Recall     F-Score \n",
      "--   --------   ---------   --------   --------\n",
      " 1     289.28      90.991     91.818     91.403                                 \n",
      " 2      54.12      92.237     91.818     92.027                                 \n",
      " 3      84.39      91.593     94.091     92.825                                 \n",
      " 4      46.45      93.213     93.636     93.424                                 \n",
      " 5      19.43      95.000     95.000     95.000                                 \n",
      " 6      15.25      93.665     94.091     93.878                                 \n",
      " 7       4.07      93.363     95.909     94.619                                 \n",
      " 8      11.23      93.722     95.000     94.357                                 \n",
      " 9      10.13      94.595     95.455     95.023                                 \n",
      "10       6.20      94.595     95.455     95.023                                 \n",
      "\u001b[1m\n",
      "============================= ✨  Results summary =============================\u001b[0m\n",
      "\n",
      "Label         Precision   Recall   F-Score\n",
      "-----------   ---------   ------   -------\n",
      "RISK_FACTOR      94.595   95.455    95.023\n",
      "\n",
      "\n",
      "Best F-Score   \u001b[38;5;2m95.023\u001b[0m\n",
      "Baseline       0.000              \n",
      "\n",
      "\u001b[38;5;2m✔ Saved model:\n",
      "/home/users/grzenkom/git/GitHub-chopeen/CORD-19/models/2020_03_29_teach/en_rf_sci_lg\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!prodigy train ner cord_19_abstracts_teach models/en_core_sci_lg_no_ner --output models/2020_03_29_teach/en_rf_sci_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Known risk factors for the disease are: older \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    age\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">RISK_FACTOR</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    male gender\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">RISK_FACTOR</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    diabetes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">RISK_FACTOR</span>\n",
       "</mark>\n",
       " and leukemia. Female patiens and children are less susceptible.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('age', 'RISK_FACTOR'),\n",
      " ('male gender', 'RISK_FACTOR'),\n",
      " ('diabetes', 'RISK_FACTOR')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Diabetes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">RISK_FACTOR</span>\n",
       "</mark>\n",
       " is a known risk factor.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Diabetes', 'RISK_FACTOR')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Leukemia is a risk factor, too.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "test_model(\"models/2020_03_29_teach/en_rf_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better model scores, but _leukemia_ still **not detected**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020.03.30 (Mon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the annotations from scratch (`strict`)\n",
    "\n",
    "Terms like _male_ or _age_ will only be marked, when they are mentioned in the context of COVID-19 risk factors, i.e. not in articles about animals or children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 label(s): RISK_FACTOR\n",
      "Added dataset cord_19_abstracts_strict to database SQLite.\n",
      "\n",
      "✨  Starting the web server at http://localhost:8080 ...\n",
      "Open the app in your browser and start annotating!\n",
      "\n",
      "^C\n",
      "\n",
      "\u001b[38;5;2m✔ Saved 41 annotations to database SQLite\u001b[0m\n",
      "Dataset: cord_19_abstracts_strict\n",
      "Session ID: 2020-03-30_20-24-26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!prodigy ner.manual cord_19_abstracts_strict models/en_core_sci_lg_no_ner data/raw/cord_19_abstracts_filtered.jsonl \\\n",
    "  --label RISK_FACTOR --patterns patterns/RF_list_2020.03.17.20037572.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded model 'models/en_core_sci_lg_no_ner'\u001b[0m\n",
      "Created and merged data for 41 total examples\n",
      "Using 21 train / 20 eval (split 50%)\n",
      "Component: ner | Batch size: compounding | Dropout: 0.2 | Iterations: 10\n",
      "\u001b[38;5;4mℹ Baseline accuracy: 0.000\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== ✨  Training the model ===========================\u001b[0m\n",
      "\n",
      "#    Loss       Precision   Recall     F-Score \n",
      "--   --------   ---------   --------   --------\n",
      " 1    1737.49       0.000      0.000      0.000                                 \n",
      " 2      25.99       0.000      0.000      0.000                                 \n",
      " 3     115.63       0.000      0.000      0.000                                 \n",
      " 4      24.86       0.000      0.000      0.000                                 \n",
      " 5      22.36       0.000      0.000      0.000                                 \n",
      " 6      18.33       0.000      0.000      0.000                                 \n",
      " 7      20.52       0.000      0.000      0.000                                 \n",
      " 8      28.45       0.000      0.000      0.000                                 \n",
      " 9      18.38       0.000      0.000      0.000                                 \n",
      "10       7.78       0.000      0.000      0.000                                 \n",
      "\u001b[1m\n",
      "============================= ✨  Results summary =============================\u001b[0m\n",
      "\n",
      "Label         Precision   Recall   F-Score\n",
      "-----------   ---------   ------   -------\n",
      "RISK_FACTOR       0.000    0.000     0.000\n",
      "\n",
      "\n",
      "Best F-Score   \u001b[38;5;2m0.000\u001b[0m\n",
      "Baseline       0.000             \n",
      "\n",
      "\u001b[38;5;2m✔ Saved model:\n",
      "/home/users/grzenkom/git/GitHub-chopeen/CORD-19/models/2020_03_30_strict/en_rf_sci_lg\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!prodigy train ner cord_19_abstracts_strict models/en_core_sci_lg_no_ner --output models/2020_03_30_strict/en_rf_sci_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the annotations from scratch (`all RF`)\n",
    "\n",
    "Highlight all risk factors - for any disease or species - but only is sentences that clear say it is a risk factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!prodigy ner.manual cord_19_abstracts_all_rf models/en_core_sci_lg_no_ner data/raw/cord_19_abstracts_filtered.jsonl \\\n",
    "  --label RISK_FACTOR --patterns patterns/RF_highlight_factor_phrases.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded model 'models/en_core_sci_lg_no_ner'\u001b[0m\n",
      "Created and merged data for 58 total examples\n",
      "Using 29 train / 29 eval (split 50%)\n",
      "Component: ner | Batch size: compounding | Dropout: 0.2 | Iterations: 10\n",
      "\u001b[38;5;4mℹ Baseline accuracy: 0.000\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== ✨  Training the model ===========================\u001b[0m\n",
      "\n",
      "#    Loss       Precision   Recall     F-Score \n",
      "--   --------   ---------   --------   --------\n",
      " 1    1960.54       0.000      0.000      0.000                                 \n",
      " 2      95.38       0.000      0.000      0.000                                 \n",
      " 3     161.78     100.000      2.128      4.167                                 \n",
      " 4      88.78      40.000      4.255      7.692                                 \n",
      " 5      97.87      33.333      4.255      7.547                                 \n",
      " 6      87.81      37.500      6.383     10.909                                 \n",
      " 7     341.72      25.000      2.128      3.922                                 \n",
      " 8     169.64      40.000      4.255      7.692                                 \n",
      " 9      79.07      30.000      6.383     10.526                                 \n",
      "10     268.87      33.333      6.383     10.714                                 \n",
      "\u001b[1m\n",
      "============================= ✨  Results summary =============================\u001b[0m\n",
      "\n",
      "Label         Precision   Recall   F-Score\n",
      "-----------   ---------   ------   -------\n",
      "RISK_FACTOR      37.500    6.383    10.909\n",
      "\n",
      "\n",
      "Best F-Score   \u001b[38;5;2m10.909\u001b[0m\n",
      "Baseline       0.000              \n",
      "\n",
      "\u001b[38;5;2m✔ Saved model:\n",
      "/home/users/grzenkom/git/GitHub-chopeen/CORD-19/models/2020_03_30_all_RF/en_rf_sci_lg\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!prodigy train ner cord_19_abstracts_all_rf models/en_core_sci_lg_no_ner --output models/2020_03_30_all_RF/en_rf_sci_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 label(s): RISK_FACTOR\n",
      "\n",
      "✨  Starting the web server at http://localhost:8081 ...\n",
      "Open the app in your browser and start annotating!\n",
      "\n",
      "^C\n",
      "\n",
      "\u001b[38;5;2m✔ Saved 7 annotations to database SQLite\u001b[0m\n",
      "Dataset: cord_19_abstracts_all_rf\n",
      "Session ID: 2020-03-30_22-01-26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!prodigy ner.teach cord_19_abstracts_all_rf models/2020_03_30_all_RF/en_rf_sci_lg data/raw/cord_19_abstracts_filtered.jsonl \\\n",
    "  --label RISK_FACTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020.04.06 (Mon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the annotations from scratch (`RF sentences`)\n",
    "\n",
    "Following the official guide:\n",
    "- https://www.youtube.com/watch?v=59BKHO_xBPA\n",
    "- https://github.com/explosion/projects/tree/master/ner-food-ingredients#data-creation-and-training-workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 label(s): RISK_FACTOR\n",
      "\n",
      "✨  Starting the web server at http://localhost:8080 ...\n",
      "Open the app in your browser and start annotating!\n",
      "\n",
      "^C\n",
      "\n",
      "\u001b[38;5;2m✔ Saved 102 annotations to database SQLite\u001b[0m\n",
      "Dataset: cord_19_rf_sentences\n",
      "Session ID: 2020-04-06_22-08-56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!prodigy ner.manual cord_19_rf_sentences blank:en data/raw/cord_19_rf_sentences.jsonl --label RISK_FACTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded model 'en_vectors_web_lg'\u001b[0m\n",
      "Created and merged data for 99 total examples\n",
      "Using 80 train / 19 eval (split 20%)\n",
      "Component: ner | Batch size: compounding | Dropout: 0.2 | Iterations: 10\n",
      "\u001b[38;5;4mℹ Baseline accuracy: 0.000\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== ✨  Training the model ===========================\u001b[0m\n",
      "\n",
      "#    Loss       Precision   Recall     F-Score \n",
      "--   --------   ---------   --------   --------\n",
      " 1     389.18       0.000      0.000      0.000                                 \n",
      " 2     191.03      25.000      4.000      6.897                                 \n",
      " 3     265.45      35.294     24.000     28.571                                 \n",
      " 4     246.57      28.571     24.000     26.087                                 \n",
      " 5     239.21      22.727     20.000     21.277                                 \n",
      " 6     215.80      42.857     24.000     30.769                                 \n",
      " 7     258.40      42.105     32.000     36.364                                 \n",
      " 8     237.19      50.000     32.000     39.024                                 \n",
      " 9     208.67      62.500     40.000     48.780                                 \n",
      "10     141.30      66.667     40.000     50.000                                 \n",
      "\u001b[1m\n",
      "============================= ✨  Results summary =============================\u001b[0m\n",
      "\n",
      "Label         Precision   Recall   F-Score\n",
      "-----------   ---------   ------   -------\n",
      "RISK_FACTOR      66.667   40.000    50.000\n",
      "\n",
      "\n",
      "Best F-Score   \u001b[38;5;2m50.000\u001b[0m\n",
      "Baseline       0.000              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!prodigy train ner cord_19_rf_sentences en_vectors_web_lg --eval-split 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded model 'en_core_web_lg'\u001b[0m\n",
      "Created and merged data for 99 total examples\n",
      "Using 80 train / 19 eval (split 20%)\n",
      "Component: ner | Batch size: compounding | Dropout: 0.2 | Iterations: 10\n",
      "\u001b[38;5;4mℹ Baseline accuracy: 0.000\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== ✨  Training the model ===========================\u001b[0m\n",
      "\n",
      "#    Loss       Precision   Recall     F-Score \n",
      "--   --------   ---------   --------   --------\n",
      " 1    1421.81       0.000      0.000      0.000                                 \n",
      " 2    1245.16       0.000      0.000      0.000                                 \n",
      " 3    1262.77       0.000      0.000      0.000                                 \n",
      " 4    1249.04       0.000      0.000      0.000                                 \n",
      " 5    1262.46       0.000      0.000      0.000                                 \n",
      " 6    1205.26       0.000      0.000      0.000                                 \n",
      " 7    1137.92       0.000      0.000      0.000                                 \n",
      " 8    1131.84       0.000      0.000      0.000                                 \n",
      " 9    1058.85       0.000      0.000      0.000                                 \n",
      "10    1143.62       0.000      0.000      0.000                                 \n",
      "\u001b[1m\n",
      "============================= ✨  Results summary =============================\u001b[0m\n",
      "\n",
      "Label         Precision   Recall   F-Score\n",
      "-----------   ---------   ------   -------\n",
      "GPE               0.000    0.000     0.000\n",
      "RISK_FACTOR       0.000    0.000     0.000\n",
      "CARDINAL          0.000    0.000     0.000\n",
      "\n",
      "\n",
      "Best F-Score   \u001b[38;5;2m0.000\u001b[0m\n",
      "Baseline       0.000             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!prodigy train ner cord_19_rf_sentences en_core_web_lg --eval-split 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⁉️ Why is `en_core_web_lg` producing no results at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded model 'en_core_sci_lg'\u001b[0m\n",
      "Created and merged data for 99 total examples\n",
      "Using 80 train / 19 eval (split 20%)\n",
      "Component: ner | Batch size: compounding | Dropout: 0.2 | Iterations: 10\n",
      "\u001b[38;5;4mℹ Baseline accuracy: 0.000\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== ✨  Training the model ===========================\u001b[0m\n",
      "\n",
      "#    Loss       Precision   Recall     F-Score \n",
      "--   --------   ---------   --------   --------\n",
      " 1     559.17       0.000      0.000      0.000                                 \n",
      " 2     550.33      33.333      8.000     12.903                                 \n",
      " 3     538.43      50.000     24.000     32.432                                 \n",
      " 4     457.89      52.632     40.000     45.455                                 \n",
      " 5     452.06      50.000     36.000     41.860                                 \n",
      " 6     415.01      60.000     48.000     53.333                                 \n",
      " 7     489.45      47.826     44.000     45.833                                 \n",
      " 8     461.13      45.000     36.000     40.000                                 \n",
      " 9     486.03      47.619     40.000     43.478                                 \n",
      "10     406.83      54.545     48.000     51.064                                 \n",
      "\u001b[1m\n",
      "============================= ✨  Results summary =============================\u001b[0m\n",
      "\n",
      "Label         Precision   Recall   F-Score\n",
      "-----------   ---------   ------   -------\n",
      "RISK_FACTOR      60.000   48.000    53.333\n",
      "\n",
      "\n",
      "Best F-Score   \u001b[38;5;2m53.333\u001b[0m\n",
      "Baseline       0.000              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!prodigy train ner cord_19_rf_sentences en_core_sci_lg --eval-split 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - pretrained models + tok2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Not using GPU\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved settings to config.json\u001b[0m\n",
      "\u001b[2K\u001b[38;5;2m✔ Loaded input texts\u001b[0m\n",
      "\u001b[2K\u001b[38;5;2m✔ Loaded model 'en_vectors_web_lg'\u001b[0m\n",
      "\u001b[1m\n",
      "============== Pre-training tok2vec layer - starting at epoch 0 ==============\u001b[0m\n",
      "  #      # Words   Total Loss     Loss    w/s\n",
      "  0        23721   23622.0449    23622   7600\n",
      "  1        47442   46297.7324    22675   8202\n",
      "  2        71163   68166.1035    21868   8409\n",
      "  3        94884   88528.3066    20362   8377\n",
      "  4       118605   107503.939    18975   8295\n",
      "  5       142326   124840.100    17336   8405\n",
      "  6       166047   141064.007    16223   8618\n",
      "  7       189768   156362.364    15298   8798\n",
      "  8       213489   170985.487    14623   8783\n",
      "  9       237210   185188.405    14202   8769\n",
      " 10       260931   198999.791    13811   8752\n",
      " 11       284652   212554.892    13555   8822\n",
      " 12       308373   225957.261    13402   8562\n",
      " 13       332094   239195.653    13238   8715\n",
      " 14       355815   252315.723    13120   8726\n",
      " 15       379536   265342.608    13026   8781\n",
      " 16       403257   278273.469    12930   8757\n",
      " 17       426978   291135.266    12861   8747\n",
      " 18       450699   303922.668    12787   8795\n",
      " 19       474420   316653.004    12730   8625\n",
      " 20       498141   329355.803    12702   8409\n",
      " 21       521862   342001.767    12645   8811\n",
      " 22       545583   354598.878    12597   8781\n",
      " 23       569304   367149.573    12550   8681\n",
      " 24       593025   379654.015    12504   8825\n",
      " 25       616746   392105.818    12451   8810\n",
      " 26       640467   404485.198    12379   8831\n",
      " 27       664188   416808.298    12323   8611\n",
      " 28       687909   429080.933    12272   8765\n",
      " 29       711630   441271.481    12190   8814\n",
      " 30       735351   453357.925    12086   8838\n",
      " 31       759072   465341.237    11983   8784\n",
      " 32       782793   477239.717    11898   8776\n",
      " 33       806514   489028.013    11788   8762\n",
      " 34       830235   500723.994    11695   8824\n",
      " 35       853956   512443.852    11719   8636\n",
      " 36       877677   524106.760    11662   8693\n",
      " 37       901398   535584.841    11478   7792\n",
      " 38       925119   547022.586    11437   8777\n",
      " 39       948840   558326.090    11303   8836\n",
      " 40       972561   569574.372    11248   8771\n",
      " 41       996282   580691.276    11116   8760\n",
      " 42      1020003   591785.128    11093   8797\n",
      " 43      1043724   602676.847    10891   8460\n",
      " 44      1067445   613498.849    10822   8715\n",
      " 45      1091166   624266.058    10767   8857\n",
      " 46      1114887   634940.039    10673   8805\n",
      " 47      1138608   645521.602    10581   8796\n",
      " 48      1162329   656106.753    10585   8783\n",
      " 49      1186050   666617.296    10510   8824\n",
      " 50      1209771   677068.398    10451   8729\n",
      " 51      1233492   687466.412    10398   8613\n",
      " 52      1257213   697890.352    10423   8824\n",
      " 53      1280934   708263.153    10372   8650\n",
      " 54      1304655   718621.868    10358   8679\n",
      " 55      1328376   728836.923    10215   8828\n",
      " 56      1352097   738986.623    10149   8807\n",
      " 57      1375818   749184.722    10198   8694\n",
      " 58      1399539   759254.917    10070   7996\n",
      " 59      1423260   769267.248    10012   8825\n",
      " 60      1446981   779261.010     9993   8833\n",
      " 61      1470702   789292.261    10031   8802\n",
      " 62      1494423   799222.415     9930   8785\n",
      " 63      1518144   809128.353     9905   8831\n",
      " 64      1541865   818978.588     9850   8792\n",
      " 65      1565586   828811.028     9832   8803\n",
      " 66      1589307   838638.685     9827   8615\n",
      " 67      1613028   848396.976     9758   8797\n",
      " 68      1636749   858134.640     9737   8784\n",
      " 69      1660470   867848.774     9714   8812\n",
      " 70      1684191   877495.642     9646   8799\n",
      " 71      1707912   887100.339     9604   8808\n",
      " 72      1731633   896707.162     9606   8796\n",
      " 73      1755354   906228.103     9520   8841\n",
      " 74      1779075   915688.896     9460   8601\n",
      " 75      1802796   925162.868     9473   8645\n",
      " 76      1826517   934643.389     9480   8784\n",
      " 77      1850238   944071.904     9428   8785\n",
      " 78      1873959   953488.285     9416   8839\n",
      " 79      1897680   962856.748     9368   8775\n",
      " 80      1921401   972179.188     9322   8754\n",
      " 81      1945122   981485.822     9306   8817\n",
      " 82      1968843   990791.101     9305   8682\n",
      " 83      1992564   1000047.91     9256   8803\n",
      " 84      2016285   1009300.21     9252   8819\n",
      " 85      2040006   1018499.48     9199   8754\n",
      " 86      2063727   1027666.34     9166   8762\n",
      " 87      2087448   1036875.26     9208   8813\n",
      " 88      2111169   1046068.54     9193   8427\n",
      " 89      2134890   1055171.28     9102   8609\n",
      " 90      2158611   1064256.24     9084   8785\n",
      " 91      2182332   1073343.92     9087   8800\n",
      " 92      2206053   1082457.17     9113   8762\n",
      " 93      2229774   1091467.76     9010   8839\n",
      " 94      2253495   1100439.91     8972   8809\n",
      " 95      2277216   1109377.63     8937   8795\n",
      " 96      2300937   1118285.97     8908   8836\n",
      " 97      2324658   1127291.36     9005   8640\n",
      " 98      2348379   1136207.87     8916   8772\n",
      " 99      2372100   1145123.50     8915   8811\n",
      "100      2395821   1153969.79     8846   8649\n",
      "101      2419542   1162899.56     8929   8803\n",
      "102      2443263   1171811.58     8912   8703\n",
      "103      2466984   1180708.57     8896   8765\n",
      "104      2490705   1189526.94     8818   8811\n",
      "105      2514426   1198323.50     8796   8625\n",
      "106      2538147   1207126.95     8803   8800\n",
      "107      2561868   1215867.16     8740   8845\n",
      "108      2585589   1224650.97     8783   8780\n",
      "109      2609310   1233397.70     8746   8714\n",
      "110      2633031   1242146.00     8748   8790\n",
      "111      2656752   1250893.56     8747   8839\n",
      "112      2680473   1259539.14     8645   8820\n",
      "113      2704194   1268171.32     8632   8659\n",
      "114      2727915   1276835.18     8663   8717\n",
      "115      2751636   1285487.13     8651   8824\n",
      "116      2775357   1294131.79     8644   8770\n",
      "117      2799078   1302734.89     8603   8807\n",
      "118      2822799   1311284.75     8549   8851\n",
      "119      2846520   1319788.61     8503   8751\n",
      "120      2870241   1328297.29     8508   8581\n",
      "121      2893962   1336858.58     8561   8777\n",
      "122      2917683   1345302.41     8443   8755\n",
      "123      2941404   1353794.27     8491   8657\n",
      "124      2965125   1362244.04     8449   8788\n",
      "125      2988846   1370680.12     8436   8819\n",
      "126      3012567   1379050.32     8370   8812\n",
      "127      3036288   1387462.94     8412   8780\n",
      "128      3060009   1395850.88     8387   8643\n",
      "129      3083730   1404266.03     8415   8724\n",
      "130      3107451   1412720.17     8454   8817\n",
      "131      3131172   1421055.33     8335   8790\n",
      "132      3154893   1429373.91     8318   8656\n",
      "133      3178614   1437656.85     8282   8674\n",
      "134      3202335   1445919.19     8262   8822\n",
      "135      3226056   1454186.20     8267   8808\n",
      "136      3249777   1462481.95     8295   8597\n",
      "137      3273498   1470755.72     8273   8765\n",
      "138      3297219   1479013.31     8257   8769\n",
      "139      3320940   1487235.98     8222   8769\n",
      "140      3344661   1495406.79     8170   8811\n",
      "141      3368382   1503611.56     8204   8800\n",
      "142      3392103   1511845.45     8233   8778\n",
      "143      3415824   1520077.15     8231   8705\n",
      "144      3439545   1528253.50     8176   8656\n",
      "145      3463266   1536387.01     8133   8806\n",
      "146      3486987   1544567.69     8180   8778\n",
      "147      3510708   1552696.62     8128   8809\n",
      "148      3534429   1560799.53     8102   8818\n",
      "149      3558150   1568886.06     8086   8676\n",
      "150      3581871   1576994.50     8108   8848\n",
      "151      3605592   1585130.12     8135   8613\n",
      "152      3629313   1593139.62     8009   8684\n",
      "153      3653034   1601226.57     8086   8797\n",
      "154      3676755   1609214.94     7988   8778\n",
      "155      3700476   1617205.89     7990   8794\n",
      "156      3724197   1625221.81     8015   8796\n",
      "157      3747918   1633160.65     7938   7233\n",
      "158      3771639   1641106.49     7945   7830\n",
      "159      3795360   1649027.41     7920   8676\n",
      "160      3819081   1656925.44     7898   8596\n",
      "161      3842802   1664874.20     7948   8697\n",
      "162      3866523   1672809.22     7935   8296\n",
      "163      3890244   1680748.19     7938   6075\n",
      "164      3913965   1688650.09     7901   6187\n",
      "165      3937686   1696544.53     7894   7160\n",
      "166      3961407   1704438.28     7893   7755\n",
      "167      3985128   1712336.05     7897   7681\n",
      "168      4008849   1720178.47     7842   7581\n",
      "169      4032570   1728056.01     7877   6380\n",
      "170      4056291   1735860.10     7804   8300\n",
      "171      4080012   1743680.62     7820   8530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172      4103733   1751515.91     7835   7146\n",
      "173      4127454   1759348.97     7833   7471\n",
      "174      4151175   1767166.22     7817   7994\n",
      "175      4174896   1774976.19     7809   7600\n",
      "176      4198617   1782727.62     7751   7573\n",
      "177      4222338   1790465.41     7737   8149\n",
      "178      4246059   1798187.30     7721   8763\n",
      "179      4269780   1805914.89     7727   8810\n",
      "180      4293501   1813677.86     7762   8786\n",
      "181      4317222   1821420.76     7742   8772\n",
      "182      4340943   1829128.08     7707   8180\n",
      "183      4364664   1836884.92     7756   8386\n",
      "184      4388385   1844550.86     7665   8479\n",
      "185      4412106   1852218.07     7667   8533\n",
      "186      4435827   1859904.95     7686   8363\n",
      "187      4459548   1867557.92     7652   8674\n",
      "188      4483269   1875238.84     7680   8744\n",
      "189      4506990   1882897.84     7659   8755\n",
      "190      4530711   1890578.88     7681   8163\n",
      "191      4554432   1898199.57     7620   6478\n",
      "192      4578153   1905808.86     7609   7709\n",
      "193      4601874   1913376.10     7567   8471\n",
      "194      4625595   1920902.62     7526   8667\n",
      "195      4649316   1928563.10     7660   8636\n",
      "196      4673037   1936122.14     7559   8708\n",
      "197      4696758   1943667.78     7545   8692\n",
      "198      4720479   1951189.56     7521   8427\n",
      "199      4744200   1958702.43     7512   8441\n",
      "200      4767921   1966288.26     7585   8230\n",
      "201      4791642   1973763.13     7474   8654\n",
      "202      4815363   1981296.87     7533   8473\n",
      "203      4839084   1988809.51     7512   8811\n",
      "204      4862805   1996281.23     7471   8789\n",
      "205      4886526   2003748.87     7467   8140\n",
      "206      4910247   2011192.45     7443   6446\n",
      "207      4933968   2018595.44     7402   7255\n",
      "208      4957689   2026031.83     7436   7049\n",
      "209      4981410   2033455.65     7423   6937\n",
      "210      5005131   2040865.45     7409   6608\n",
      "211      5028852   2048289.12     7423   7015\n",
      "212      5052573   2055700.70     7411   7027\n",
      "213      5076294   2063150.58     7449   6425\n",
      "214      5100015   2070530.65     7380   5505\n",
      "215      5123736   2077896.01     7365   5630\n",
      "216      5147457   2085272.02     7376   5398\n",
      "217      5171178   2092660.66     7388   5408\n",
      "218      5194899   2100025.57     7364   4984\n",
      "219      5218620   2107355.34     7329   5174\n",
      "220      5242341   2114752.74     7397   5962\n",
      "221      5266062   2122127.21     7374   5586\n",
      "222      5289783   2129459.69     7332   5492\n",
      "223      5313504   2136793.77     7334   5177\n",
      "224      5337225   2144088.41     7294   5810\n",
      "225      5360946   2151392.68     7304   7329\n",
      "226      5384667   2158645.93     7253   1130\n",
      "227      5408388   2165868.09     7222   1459\n",
      "228      5432109   2173169.07     7300   8298\n",
      "229      5455830   2180420.89     7251   7589\n",
      "230      5479551   2187706.64     7285   8354\n",
      "231      5503272   2194983.08     7276   7482\n",
      "232      5526993   2202263.46     7280   6118\n",
      "233      5550714   2209554.28     7290   6751\n",
      "234      5574435   2216754.11     7199   8416\n",
      "235      5598156   2224035.09     7280   8336\n",
      "236      5621877   2231249.56     7214   8409\n",
      "237      5645598   2238445.04     7195   8210\n",
      "238      5669319   2245640.48     7195   8344\n",
      "239      5693040   2252901.44     7260   8182\n",
      "240      5716761   2260078.30     7176   8100\n",
      "241      5740482   2267237.75     7159   6976\n",
      "242      5764203   2274387.89     7150   8019\n",
      "243      5787924   2281572.83     7184   7427\n",
      "244      5811645   2288724.01     7151   7219\n",
      "245      5835366   2295882.51     7158   8163\n",
      "246      5859087   2303017.66     7135   8454\n",
      "247      5882808   2310144.69     7127   8449\n",
      "248      5906529   2317253.53     7108   8249\n",
      "249      5930250   2324345.90     7092   7452\n",
      "250      5953971   2331485.69     7139   6231\n",
      "251      5977692   2338605.80     7120   7052\n",
      "252      6001413   2345636.11     7030   7646\n",
      "253      6025134   2352752.67     7116   7304\n",
      "254      6048855   2359838.68     7086   7232\n",
      "255      6072576   2366909.94     7071   5143\n",
      "256      6096297   2374012.92     7102   4737\n",
      "257      6120018   2381102.69     7089   5313\n",
      "258      6143739   2388171.54     7068   6481\n",
      "259      6167460   2395169.16     6997   6484\n",
      "260      6191181   2402193.34     7024   6073\n",
      "261      6214902   2409165.41     6972   5567\n",
      "262      6238623   2416193.64     7028   6061\n",
      "263      6262344   2423184.12     6990   6204\n",
      "264      6286065   2430117.50     6933   5876\n",
      "265      6309786   2437097.74     6980   5728\n",
      "266      6333507   2444127.76     7030   5096\n",
      "267      6357228   2451106.65     6978   8243\n",
      "268      6380949   2458088.14     6981   8566\n",
      "269      6404670   2465144.64     7056   8344\n",
      "270      6428391   2472152.57     7007   8359\n",
      "271      6452112   2479134.51     6981   8717\n",
      "272      6475833   2486159.83     7025   8600\n",
      "273      6499554   2493142.07     6982   8345\n",
      "274      6523275   2500041.14     6899   8475\n",
      "275      6546996   2506951.47     6910   8613\n",
      "276      6570717   2513889.30     6937   8429\n",
      "277      6594438   2520809.51     6920   8600\n",
      "278      6618159   2527687.43     6877   8776\n",
      "279      6641880   2534611.67     6924   8575\n",
      "280      6665601   2541536.70     6925   8662\n",
      "281      6689322   2548418.29     6881   8636\n",
      "282      6713043   2555321.85     6903   8687\n",
      "283      6736764   2562171.11     6849   8464\n",
      "284      6760485   2569134.68     6963   8608\n",
      "285      6784206   2576028.49     6893   8217\n",
      "286      6807927   2582977.93     6949   8612\n",
      "287      6831648   2589833.92     6855   8583\n",
      "288      6855369   2596705.94     6872   8674\n",
      "289      6879090   2603556.74     6850   8719\n",
      "290      6902811   2610398.24     6841   8466\n",
      "291      6926532   2617240.68     6842   8345\n",
      "292      6950253   2624042.99     6802   8682\n",
      "293      6973974   2630871.85     6828   8711\n",
      "294      6997695   2637691.02     6819   8689\n",
      "295      7021416   2644550.10     6859   8488\n",
      "296      7045137   2651315.06     6764   8607\n",
      "297      7068858   2658107.17     6792   8096\n",
      "298      7092579   2664921.39     6814   8555\n",
      "299      7116300   2671700.01     6778   8461\n",
      "300      7140021   2678513.43     6813   8673\n",
      "301      7163742   2685279.54     6766   8651\n",
      "302      7187463   2692055.09     6775   8532\n",
      "303      7211184   2698818.06     6762   8546\n",
      "304      7234905   2705598.94     6780   8649\n",
      "305      7258626   2712322.87     6723   8654\n",
      "306      7282347   2719059.19     6736   8634\n",
      "307      7306068   2725865.72     6806   8352\n",
      "308      7329789   2732566.20     6700   8554\n",
      "309      7353510   2739268.27     6702   8440\n",
      "310      7377231   2745993.42     6725   8213\n",
      "311      7400952   2752769.94     6776   8126\n",
      "312      7424673   2759511.38     6741   7603\n",
      "313      7448394   2766214.92     6703   8105\n",
      "314      7472115   2772976.18     6761   7298\n",
      "315      7495836   2779669.12     6692   6430\n",
      "316      7519557   2786356.95     6687   7740\n",
      "317      7543278   2793120.06     6763   8506\n",
      "318      7566999   2799813.61     6693   8480\n",
      "319      7590720   2806482.26     6668   8415\n",
      "320      7614441   2813131.87     6649   8496\n",
      "321      7638162   2819799.93     6668   8370\n",
      "322      7661883   2826475.13     6675   6866\n",
      "323      7685604   2833121.93     6646   7323\n",
      "324      7709325   2839774.94     6653   7820\n",
      "325      7733046   2846493.58     6718   7561\n",
      "326      7756767   2853150.05     6656   6733\n",
      "327      7780488   2859807.66     6657   7193\n",
      "328      7804209   2866467.45     6659   7142\n",
      "329      7827930   2873098.40     6630   6783\n",
      "330      7851651   2879723.52     6625   6825\n",
      "331      7875372   2886291.03     6567   6943\n",
      "332      7899093   2892890.43     6599   6781\n",
      "333      7922814   2899507.33     6616   6962\n",
      "334      7946535   2906162.32     6654   6855\n",
      "335      7970256   2912805.98     6643   6107\n",
      "336      7993977   2919373.68     6567   5395\n",
      "337      8017698   2925984.91     6611   5063\n",
      "338      8041419   2932557.63     6572   4913\n",
      "339      8065140   2939095.27     6537   5010\n",
      "340      8088861   2945674.50     6579   5120\n",
      "341      8112582   2952287.75     6613   4966\n",
      "342      8136303   2958907.02     6619   5926\n",
      "343      8160024   2965513.72     6606   5631\n",
      "344      8183745   2972104.91     6591   5464\n",
      "345      8207466   2978677.69     6572   5758\n",
      "346      8231187   2985256.56     6578   7519\n",
      "347      8254908   2991820.86     6564   8730\n",
      "348      8278629   2998384.39     6563   8496\n",
      "349      8302350   3004912.36     6527   8283\n",
      "350      8326071   3011457.29     6544   7952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351      8349792   3017978.89     6521   8557\n",
      "352      8373513   3024520.78     6541   8461\n",
      "353      8397234   3031020.49     6499   8240\n",
      "354      8420955   3037543.94     6523   7204\n",
      "355      8444676   3044064.91     6520   7793\n",
      "356      8468397   3050565.23     6500   7162\n",
      "357      8492118   3057064.23     6499   8085\n",
      "358      8515839   3063619.42     6555   8258\n",
      "359      8539560   3070146.39     6526   8492\n",
      "360      8563281   3076676.22     6529   8359\n",
      "361      8587002   3083193.01     6516   8328\n",
      "362      8610723   3089662.92     6469   8298\n",
      "363      8634444   3096207.45     6544   7768\n",
      "364      8658165   3102677.79     6470   7721\n",
      "365      8681886   3109112.09     6434   7968\n",
      "366      8705607   3115594.99     6482   7957\n",
      "367      8729328   3122124.48     6529   6957\n",
      "368      8753049   3128587.03     6462   6439\n",
      "369      8776770   3135034.63     6447   6336\n",
      "370      8800491   3141431.78     6397   5543\n",
      "371      8824212   3147879.98     6448   5785\n",
      "372      8847933   3154326.60     6446   6678\n",
      "373      8871654   3160749.49     6422   6725\n",
      "374      8895375   3167172.95     6423   6379\n",
      "375      8919096   3173589.08     6416   6539\n",
      "376      8942817   3180045.45     6456   6571\n",
      "377      8966538   3186476.75     6431   5963\n",
      "378      8990259   3192898.14     6421   6324\n",
      "379      9013980   3199267.63     6369   7308\n",
      "380      9037701   3205677.17     6409   7966\n",
      "381      9061422   3212080.77     6403   8629\n",
      "382      9085143   3218502.00     6421   8355\n",
      "383      9108864   3224918.06     6416   8263\n",
      "384      9132585   3231315.00     6396   6869\n",
      "385      9156306   3237671.29     6356   8496\n",
      "386      9180027   3244065.43     6394   8023\n",
      "387      9203748   3250409.05     6343   7305\n",
      "388      9227469   3256799.56     6390   8151\n",
      "389      9251190   3263183.95     6384   8388\n",
      "390      9274911   3269622.72     6438   8566\n",
      "391      9298632   3275965.64     6342   8470\n",
      "392      9322353   3282359.36     6393   8348\n",
      "393      9346074   3288739.09     6379   8444\n",
      "394      9369795   3295136.50     6397   8503\n",
      "395      9393516   3301480.17     6343   8479\n",
      "396      9417237   3307828.31     6348   8316\n",
      "397      9440958   3314185.05     6356   8206\n",
      "398      9464679   3320557.83     6372   8425\n",
      "399      9488400   3326873.77     6315   7802\n",
      "400      9512121   3333246.69     6372   7476\n",
      "401      9535842   3339582.04     6335   7970\n",
      "402      9559563   3345921.23     6339   8345\n",
      "403      9583284   3352287.94     6366   8399\n",
      "404      9607005   3358575.37     6287   8566\n",
      "405      9630726   3364861.29     6285   8660\n",
      "406      9654447   3371193.67     6332   8743\n",
      "407      9678168   3377476.70     6283   8564\n",
      "408      9701889   3383785.37     6308   8525\n",
      "409      9725610   3390098.07     6312   8704\n",
      "410      9749331   3396394.95     6296   8713\n",
      "411      9773052   3402714.28     6319   8590\n",
      "412      9796773   3409029.24     6314   8626\n",
      "413      9820494   3415317.51     6288   8525\n",
      "414      9844215   3421613.79     6296   8585\n",
      "415      9867936   3427848.81     6235   8518\n",
      "416      9891657   3434139.23     6290   8683\n",
      "417      9915378   3440400.14     6260   8708\n",
      "418      9939099   3446680.64     6280   8654\n",
      "419      9962820   3452951.23     6270   8741\n",
      "420      9986541   3459196.05     6244   7891\n",
      "421     10010262   3465422.22     6226   8364\n",
      "422     10033983   3471689.08     6266   8551\n",
      "423     10057704   3477926.60     6237   8340\n",
      "424     10081425   3484166.05     6239   8528\n",
      "425     10105146   3490446.75     6280   8457\n",
      "426     10128867   3496702.28     6255   8408\n",
      "427     10152588   3502971.08     6268   8335\n",
      "428     10176309   3509257.19     6286   8489\n",
      "429     10200030   3515474.90     6217   8311\n",
      "430     10223751   3521715.82     6240   8458\n",
      "431     10247472   3527947.02     6231   7896\n",
      "432     10271193   3534156.37     6209   8282\n",
      "433     10294914   3540375.14     6218   8282\n",
      "434     10318635   3546569.90     6194   8222\n",
      "435     10342356   3552773.95     6204   8181\n",
      "436     10366077   3558920.50     6146   8423\n",
      "437     10389798   3565152.92     6232   8096\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/users/grzenkom/apps/miniconda3/envs/cord-19-env/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/users/grzenkom/apps/miniconda3/envs/cord-19-env/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/users/grzenkom/apps/miniconda3/envs/cord-19-env/lib/python3.7/site-packages/spacy/__main__.py\", line 33, in <module>\n",
      "    plac.call(commands[command], sys.argv[1:])\n",
      "  File \"/home/users/grzenkom/apps/miniconda3/envs/cord-19-env/lib/python3.7/site-packages/plac_core.py\", line 328, in call\n",
      "    cmd, result = parser.consume(arglist)\n",
      "  File \"/home/users/grzenkom/apps/miniconda3/envs/cord-19-env/lib/python3.7/site-packages/plac_core.py\", line 207, in consume\n",
      "    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n",
      "  File \"/home/users/grzenkom/apps/miniconda3/envs/cord-19-env/lib/python3.7/site-packages/spacy/cli/pretrain.py\", line 233, in pretrain\n",
      "    model, docs, optimizer, objective=loss_func, drop=dropout\n",
      "  File \"/home/users/grzenkom/apps/miniconda3/envs/cord-19-env/lib/python3.7/site-packages/spacy/cli/pretrain.py\", line 262, in make_update\n",
      "    backprop(gradients, sgd=optimizer)\n",
      "  File \"/home/users/grzenkom/apps/miniconda3/envs/cord-19-env/lib/python3.7/site-packages/spacy/_ml.py\", line 840, in mlm_backward\n",
      "    return backprop(d_output, sgd=sgd)\n",
      "  File \"/home/users/grzenkom/apps/miniconda3/envs/cord-19-env/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 53, in continue_update\n",
      "    gradient = callback(gradient, sgd)\n",
      "  File \"/home/users/grzenkom/apps/miniconda3/envs/cord-19-env/lib/python3.7/site-packages/thinc/api.py\", line 300, in finish_update\n",
      "    d_X = bp_layer(layer.ops.flatten(d_seqs_out, pad=pad), sgd=sgd)\n",
      "  File \"/home/users/grzenkom/apps/miniconda3/envs/cord-19-env/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 53, in continue_update\n",
      "    gradient = callback(gradient, sgd)\n",
      "  File \"/home/users/grzenkom/apps/miniconda3/envs/cord-19-env/lib/python3.7/site-packages/thinc/neural/_classes/resnet.py\", line 37, in residual_bwd\n",
      "    dX = bp_y(d_output, sgd)\n",
      "  File \"/home/users/grzenkom/apps/miniconda3/envs/cord-19-env/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 53, in continue_update\n",
      "    gradient = callback(gradient, sgd)\n",
      "  File \"ops.pyx\", line 128, in thinc.neural.ops.Ops.dropout.wrap_backprop.finish_update\n",
      "  File \"/home/users/grzenkom/apps/miniconda3/envs/cord-19-env/lib/python3.7/site-packages/thinc/neural/_classes/layernorm.py\", line 76, in finish_update\n",
      "    return backprop_child(d_xhat, sgd)\n",
      "  File \"/home/users/grzenkom/apps/miniconda3/envs/cord-19-env/lib/python3.7/site-packages/thinc/neural/_classes/maxout.py\", line 86, in finish_update\n",
      "    d_W = self.ops.gemm(dX__bop, X__bi, trans1=True)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!spacy pretrain data/raw/cord_19_rf_sentences.jsonl en_vectors_web_lg models/tok2vec_rf --use-vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model `460` will be used for futher processing - `tok2vec_rf_model460.bin`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
